# Module 5: Quality & Verification

> **Applies to both API and CLI**: These quality patterns work whether you're using Codex programmatically or through the interactive CLI. The examples show CLI usage, but the principles transfer directly to API integrations.

## Overview

Quality isn't something you add at the endâ€”it's built into every step. This module teaches you systematic approaches to catch issues early, maintain high standards, and build confidence in your code changes.

**Learning Objectives**:
- Implement multi-layer review patterns that catch different types of issues
- Apply test-driven development with Codex as your pair programmer
- Debug systematically instead of guessing
- Create quality gates that prevent bad code from progressing
- Master root cause analysis techniques

**Time**: 3-4 hours

---

## 1. Multi-Layer Review Patterns

### Why Multiple Layers?

Different review methods catch different problems:
- **Automated tools** catch syntax, formatting, and known anti-patterns instantly
- **AI review** catches logic errors, edge cases, and suggests improvements
- **Human review** catches architecture issues, business logic errors, and UX problems

No single layer catches everything. Stack them.

```mermaid
flowchart LR
    subgraph layer1["âš¡ LAYER 1: AUTOMATED"]
        L1["Formatters"]
        L2["Linters"]
        L3["Type Check"]
    end

    subgraph layer2["ðŸ¤– LAYER 2: AI-ASSISTED"]
        A1["Code Review"]
        A2["Edge Cases"]
        A3["Coverage"]
    end

    subgraph layer3["ðŸ§  LAYER 3: HUMAN"]
        M1["Architecture"]
        M2["Business Logic"]
        M3["UX / Security"]
    end

    CODE["ðŸ“ Code"] --> layer1 --> layer2 --> layer3 --> MERGE["âœ… Merge"]

    style layer1 fill:#e8f5e9,stroke:#2e7d32
    style layer2 fill:#e3f2fd,stroke:#1565c0
    style layer3 fill:#fce4ec,stroke:#c2185b
```

### Layer 1: Automated Checks (Instant Feedback)

These run automatically via hooksâ€”you don't even think about them.

**Setup once, benefit forever:**
```json
{
  "hooks": {
    "Write:Callback": {
      "command": "prettier",
      "args": ["--write", "$FILE_PATH"]
    },
    "Write:Validate": {
      "command": "eslint",
      "args": ["--quiet", "$FILE_PATH"]
    }
  }
}
```

**What this catches:**
| Tool | Catches | Example |
|------|---------|---------|
| Prettier | Inconsistent formatting | Mixed tabs/spaces, line length |
| ESLint | Code smells, bugs | Unused variables, missing returns |
| TypeScript | Type errors | Wrong argument types, null access |
| Pre-commit | Policy violations | Console.logs, debug code |

### Layer 2: AI-Assisted Review (Smart Analysis)

This is where Codex shines. Ask it to review with specific focus areas.

**Effective code review prompt:**
```bash
codex "Review the changes I just made. Focus on:

1. LOGIC: Are there any bugs or incorrect assumptions?
2. EDGE CASES: What inputs would break this code?
3. ERROR HANDLING: Are failures handled gracefully?
4. SECURITY: Any vulnerabilities (injection, auth bypass)?

Show the git diff first, then give specific feedback with line numbers."
```

**Example conversation:**
```
You: Review my changes for the login function

Codex: Looking at your changes to src/auth/login.ts:

âš ï¸ LOGIC ISSUE (line 34):
You're checking `user.isActive` after password validation.
If user is inactive, you still leak that the password was correct.
â†’ Check `isActive` before password comparison.

âš ï¸ EDGE CASE (line 42):
No handling for when `user.email` is null/undefined.
â†’ Add null check or use optional chaining.

âœ… GOOD: Password comparison uses timing-safe comparison.
âœ… GOOD: Failed attempts are rate-limited.

Suggested fix for line 34:
- if (user && await bcrypt.compare(password, user.hash)) {
-   if (!user.isActive) return { error: 'Account disabled' };
+ if (user && user.isActive && await bcrypt.compare(password, user.hash)) {
```

**ðŸ’¡ Pro Tip**: Be specific about what you want reviewed. "Review my code" gives generic feedback. "Review for SQL injection vulnerabilities" gives targeted, useful feedback.

### Using Code Review Skills

For consistent, thorough reviews, use the superpowers code review skills instead of ad-hoc prompts.

#### Requesting Reviews: `superpowers:requesting-code-review`

This skill provides a structured review workflow that ensures nothing gets missed:

```bash
codex "Use superpowers:requesting-code-review on my changes"
```

**What the skill does:**
1. Gathers your changes (staged, unstaged, or specific files)
2. Runs through a comprehensive checklist:
   - Logic errors and bugs
   - Edge cases and boundary conditions
   - Error handling completeness
   - Security vulnerabilities
   - Code style and conventions
   - Test coverage gaps
3. Reports findings with severity levels
4. Suggests specific fixes

**Example session:**
```
You: I've finished the payment integration. Use superpowers:requesting-code-review

Codex: Running code review on payment integration...

ðŸ“‹ REVIEW SUMMARY

ðŸ”´ CRITICAL (1):
- src/payments/processor.ts:67 - API key exposed in error message
  â†’ Remove sensitive data from user-facing errors

ðŸŸ¡ WARNINGS (2):
- src/payments/processor.ts:34 - No timeout on external API call
  â†’ Add timeout to prevent hanging requests
- src/payments/validator.ts:12 - Decimal precision loss possible
  â†’ Use decimal library for currency calculations

ðŸŸ¢ SUGGESTIONS (1):
- Consider adding idempotency key for retry safety

ðŸ“Š Coverage: 3 new functions, 2 have tests, 1 missing tests

Shall I fix the critical issue first?
```

#### Receiving Feedback: `superpowers:receiving-code-review`

When someone (human or AI) reviews your code, this skill helps you process feedback properly:

```bash
codex "Use superpowers:receiving-code-review for this feedback: [paste feedback]"
```

**What the skill does:**
1. Parses and categorizes the feedback
2. Identifies which comments require changes vs. discussion
3. Helps you evaluate suggestions critically (not all feedback should be accepted)
4. Creates a plan to address valid points
5. Drafts responses for items you disagree with

**Why this matters:**

| Without Skill | With Skill |
|---------------|------------|
| Blindly accept all suggestions | Critically evaluate each point |
| Miss subtle feedback | Categorize by type and priority |
| No systematic response | Structured plan to address |
| Forget to follow up | Track what's resolved |

**Example: Processing PR feedback**
```
You: Use superpowers:receiving-code-review for this feedback:
"The error handling looks fragile. Also, why not use the existing
UserService instead of creating a new one? Minor: typo on line 34."

Codex: Analyzing feedback...

ðŸ“‹ FEEDBACK BREAKDOWN

1. "Error handling looks fragile" - REQUIRES INVESTIGATION
   â†’ Vague feedback. Need to understand what specifically is fragile.
   â†’ Suggested response: "Could you point to specific error cases you're concerned about?"

2. "Use existing UserService" - REQUIRES EVALUATION
   â†’ Valid architectural question. Let me check...
   â†’ Found: UserService exists at src/services/user.ts
   â†’ Assessment: The reviewer is right. UserService already handles user lookup.
   â†’ Action: Refactor to use UserService instead of duplicating logic.

3. "Typo on line 34" - QUICK FIX
   â†’ Action: Fix typo

Plan:
1. [ ] Ask for clarification on error handling concern
2. [ ] Refactor to use UserService
3. [ ] Fix typo
4. [ ] Re-request review after changes
```

#### When to Use Each Skill

| Situation | Skill | Why |
|-----------|-------|-----|
| Before creating a PR | `requesting-code-review` | Catch issues before others see them |
| After receiving PR comments | `receiving-code-review` | Process feedback systematically |
| Self-review of complex changes | `requesting-code-review` | Fresh perspective on your own code |
| Disagreeing with feedback | `receiving-code-review` | Formulate thoughtful responses |

> **See Module 3** for installation and more details on using superpowers skills.

### Layer 3: Manual Review (Human Judgment)

Humans excel at questions AI can't answer:
- Does this match what the product team asked for?
- Will users understand this interface?
- Does this fit our architecture vision?
- Are we solving the right problem?

**Your manual review checklist:**
```markdown
- [ ] Does this solve the actual user problem?
- [ ] Will this scale to 10x current usage?
- [ ] Can a new team member understand this in 6 months?
- [ ] Does this introduce technical debt we'll regret?
- [ ] Are there compliance/legal considerations?
```

### The Complete Review Workflow

```bash
# 1. Make your changes
codex "Add rate limiting to the /api/login endpoint"

# 2. Automated checks run via hooks (you see errors immediately)

# 3. AI review for logic and edge cases
codex "Review my rate limiting implementation:
- Is the algorithm correct?
- What happens at boundary conditions?
- Can it be bypassed?"

# 4. Fix issues found
codex "Fix the bypass vulnerability you identified"

# 5. Verify test coverage
codex "What tests should I add for the rate limiter?"

# 6. Add missing tests
codex "Add tests for the edge cases you identified"

# 7. Final human review - you look at the PR yourself
codex "Summarize all changes for my PR description"
```

---

## 2. Test-Driven Development with Codex

### The TDD Cycle

TDD isn't about testingâ€”it's about design. Writing tests first forces you to think about the interface before the implementation.

```mermaid
flowchart LR
    RED["ðŸ”´ RED\nWrite failing test"]
    GREEN["ðŸŸ¢ GREEN\nMinimal code to pass"]
    REFACTOR["ðŸ”µ REFACTOR\nImprove quality"]

    RED --> GREEN --> REFACTOR --> RED

    style RED fill:#ffebee,stroke:#c62828,stroke-width:2px
    style GREEN fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style REFACTOR fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
```

### TDD with Codex: A Complete Example

Let's build an email validator using TDD:

**ðŸ”´ RED: Write the failing test first**
```bash
codex "I need to validate email addresses. Write the test first,
following TDD. Don't implement yetâ€”just the test."
```

Codex writes:
```typescript
// tests/emailValidator.test.ts
describe('validateEmail', () => {
  it('accepts valid email addresses', () => {
    expect(validateEmail('user@example.com')).toBe(true);
    expect(validateEmail('name.surname@company.co.uk')).toBe(true);
  });

  it('rejects invalid email addresses', () => {
    expect(validateEmail('not-an-email')).toBe(false);
    expect(validateEmail('@missing-local.com')).toBe(false);
    expect(validateEmail('missing-domain@')).toBe(false);
  });

  it('rejects empty and null inputs', () => {
    expect(validateEmail('')).toBe(false);
    expect(validateEmail(null)).toBe(false);
    expect(validateEmail(undefined)).toBe(false);
  });
});
```

```bash
# Run test - it fails because validateEmail doesn't exist
npm test
# âŒ FAIL: validateEmail is not defined
```

**ðŸŸ¢ GREEN: Implement minimal code to pass**
```bash
codex "Now implement validateEmail with the minimum code to pass these tests."
```

Codex writes:
```typescript
// src/emailValidator.ts
export function validateEmail(email: unknown): boolean {
  if (typeof email !== 'string' || !email) return false;
  return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(email);
}
```

```bash
npm test
# âœ… PASS: All tests passing
```

**ðŸ”µ REFACTOR: Improve without breaking tests**
```bash
codex "The tests pass. Now refactor for clarity and add edge cases we missed."
```

Codex responds:
```typescript
// Improved with constants and better organization
const EMAIL_REGEX = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
const MAX_EMAIL_LENGTH = 254; // RFC 5321

export function validateEmail(email: unknown): boolean {
  if (typeof email !== 'string') return false;
  if (email.length === 0 || email.length > MAX_EMAIL_LENGTH) return false;
  return EMAIL_REGEX.test(email);
}

// Additional tests I'd recommend:
// - Email at max length (254 chars)
// - Email just over max length
// - Unicode characters in local part
// - Multiple @ symbols
```

### Edge Case Discovery Pattern

After basic implementation, ask Codex to find what you missed:

```bash
codex "What edge cases might break my email validator?
List inputs that a real attacker or buggy system might send."
```

Response:
```
Edge cases to consider:

1. UNICODE ATTACKS:
   - 'user@Ã©xample.com' (IDN domains)
   - 'Ã¼ser@example.com' (unicode in local part)

2. LENGTH ATTACKS:
   - 'a'*1000 + '@example.com' (very long local part)
   - 'a@' + 'b'*1000 + '.com' (very long domain)

3. SPECIAL CHARACTERS:
   - '"user name"@example.com' (quoted local part - valid!)
   - 'user+tag@example.com' (plus addressing)
   - 'user@[192.168.1.1]' (IP address domain)

4. BOUNDARY CASES:
   - 'a@b.c' (minimum valid email)
   - '.user@example.com' (leading dot)
   - 'user.@example.com' (trailing dot)

Shall I add tests for these?
```

**ðŸ’¡ Pro Tip**: Always ask "What would break this?" after implementing. Codex is great at adversarial thinking.

---

## 3. Systematic Debugging

### The 3-Attempt Rule

> **After 3 failed fix attempts, STOP GUESSING and switch to systematic debugging.**

Guessing wastes time and often introduces new bugs. Here's the pattern:

```mermaid
flowchart TD
    BUG["ðŸ› Bug Found"] --> A1["Try Fix #1"]
    A1 --> C1{Works?}
    C1 -->|Yes| DONE["âœ… Fixed!"]
    C1 -->|No| A2["Try Fix #2"]
    A2 --> C2{Works?}
    C2 -->|Yes| DONE
    C2 -->|No| A3["Try Fix #3"]
    A3 --> C3{Works?}
    C3 -->|Yes| DONE
    C3 -->|No| STOP["ðŸ›‘ STOP GUESSING"]
    STOP --> SYS["ðŸ“‹ Systematic Debug"]

    style STOP fill:#ffcdd2,stroke:#c62828
    style SYS fill:#e3f2fd,stroke:#1565c0
    style DONE fill:#c8e6c9,stroke:#2e7d32
```

### Systematic Debugging Framework

When you hit the wall after 3 attempts, use this framework:

**Phase 1: Reproduce Reliably**
```bash
codex "I'm getting intermittent 500 errors on /api/users. Help me create
a reliable reproduction case. What information do you need?"
```

Codex will ask for:
- Exact request that fails (curl command, headers, body)
- Error logs/stack trace
- When it started (recent changes?)
- Pattern (every time? Only certain users? Only under load?)

**Phase 2: Trace Backwards**
```bash
codex "Here's the stack trace: [paste trace]
Trace backwards from the error. What data becomes invalid and where?"
```

Example response:
```
Tracing backwards through your stack trace:

ERROR: Cannot read property 'email' of undefined
  at UserService.getProfile (user-service.ts:45)
  at UserController.show (user-controller.ts:23)
  at Router.handle (router.ts:156)

ðŸ” ANALYSIS:
Line 45: `return user.email` - 'user' is undefined
Line 38: `const user = await this.repo.findById(id)`
Line 23: `const id = req.params.id`

HYPOTHESIS: The user ID from params isn't finding a user.

Questions to investigate:
1. Is `id` the correct format? (string vs number)
2. Does this user exist in the database?
3. Is there a race condition where user is deleted mid-request?

To test, add logging at line 38:
console.log('Looking for user:', id, typeof id);
```

**Phase 3: Form and Test Hypothesis**
```bash
codex "I added logging. The ID is '123' (string) but the database
expects a number. How should I fix this properly?"
```

**Phase 4: Fix and Prevent Regression**
```bash
codex "Fix the type mismatch and add a test that would have caught this."
```

### Debugging Patterns

**Pattern: Binary Search with Git Bisect**

When "it worked yesterday":
```bash
codex "The login feature broke sometime this week. Help me use git bisect
to find the breaking commit. What commands do I run?"
```

Response:
```bash
# Start bisect
git bisect start

# Mark current (broken) as bad
git bisect bad

# Mark last known good commit (e.g., last Monday)
git bisect good abc123

# Git checks out a middle commit. Test it:
npm test

# Tell git the result
git bisect good  # or 'git bisect bad'

# Repeat until git identifies the breaking commit
# Usually finds it in ~7 steps for 100 commits
```

**Pattern: Isolation Testing**

When you don't know which component is failing:
```bash
codex "My API returns wrong data. I don't know if the bug is in:
- The controller
- The service layer
- The database query
- The serializer

Help me test each layer in isolation."
```

---

## 4. Proactive Quality Gates

Quality gates prevent bad code from progressing. Each gate must pass before moving forward.

```mermaid
flowchart LR
    subgraph gate1["ðŸ“‹ GATE 1"]
        G1["Pre-Implementation"]
        G1a["Requirements clear?"]
        G1b["Edge cases identified?"]
    end

    subgraph gate2["ðŸ”¨ GATE 2"]
        G2["During Implementation"]
        G2a["Tests with code?"]
        G2b["Small commits?"]
    end

    subgraph gate3["âœ… GATE 3"]
        G3["Pre-Commit"]
        G3a["All tests pass?"]
        G3b["Lint clean?"]
    end

    subgraph gate4["ðŸš€ GATE 4"]
        G4["Pre-PR"]
        G4a["Coverage adequate?"]
        G4b["Docs updated?"]
    end

    START["ðŸ’¡ Idea"] --> gate1 --> gate2 --> gate3 --> gate4 --> MERGE["ðŸŽ‰ Merged!"]

    style gate1 fill:#fff3e0,stroke:#e65100
    style gate2 fill:#e8f5e9,stroke:#2e7d32
    style gate3 fill:#e3f2fd,stroke:#1565c0
    style gate4 fill:#f3e5f5,stroke:#7b1fa2
```

### Gate 1: Pre-Implementation

Before writing code, verify you understand the problem:

```bash
codex "I'm about to implement user notifications. Before I start:

1. What questions should I answer first?
2. What edge cases should I consider?
3. What's the simplest version I could ship?"
```

**Checklist:**
- [ ] Requirements are written down (not just in your head)
- [ ] Edge cases identified
- [ ] Test strategy defined
- [ ] Breaking changes identified

### Gate 2: During Implementation

While coding, maintain quality continuously:

```bash
# Every 30 minutes or after each feature chunk:
codex "Quick check on my progress:
- Am I following TDD?
- Any obvious issues in what I've written?
- Am I overcomplicating this?"
```

**Checklist:**
- [ ] Tests written before/with implementation
- [ ] Commits are small and focused
- [ ] No commented-out code accumulating
- [ ] No TODO without a ticket number

### Gate 3: Pre-Commit

Before committing, run comprehensive checks:

```bash
# Parallel review before commit
codex exec "Review my staged changes for bugs" > review.txt &
codex exec "Check test coverage for staged changes" > coverage.txt &
wait

# Check results
cat review.txt coverage.txt

# If clean, commit
git commit -m "Add user notifications"
```

**Automated gate with hooks:**
```json
{
  "hooks": {
    "Bash:Validate": {
      "command": "sh",
      "args": ["-c", "npm test && npm run lint && npm run typecheck"]
    }
  }
}
```

### Gate 4: Pre-PR

Before creating a PR, ensure it's ready for others:

```bash
codex "I'm about to create a PR. Check:

1. Do all tests pass?
2. Is there adequate test coverage?
3. Is the PR description clear?
4. Are there any debugging artifacts to remove?

Here's my diff: $(git diff main...HEAD)"
```

**ðŸ’¡ Pro Tip**: Create a personal pre-PR checklist and save it. Run it every time:

```bash
#!/bin/bash
# scripts/pre-pr-check.sh

echo "ðŸ” Running pre-PR checks..."

echo "1. Running tests..."
npm test || exit 1

echo "2. Running linter..."
npm run lint || exit 1

echo "3. Checking for debug code..."
if grep -r "console.log\|debugger" src/; then
  echo "âŒ Found debug code!"
  exit 1
fi

echo "4. Checking for TODOs without tickets..."
if grep -r "TODO:" src/ | grep -v "TODO(#"; then
  echo "âš ï¸  Found TODOs without ticket numbers"
fi

echo "âœ… All checks passed!"
```

---

## 5. Root Cause Analysis

### The 5 Whys Technique

Don't fix symptomsâ€”fix causes. Ask "why" until you reach the root:

```
ðŸ› Bug: Users can't log in

Why? â†’ The auth token is invalid
Why? â†’ The token expired
Why? â†’ Token refresh didn't run
Why? â†’ The refresh timer was cleared
Why? â†’ Component unmount clears all timers
ROOT CAUSE: Need to persist refresh timer or use a different approach
```

```bash
codex "Help me apply the 5 Whys to this bug: [describe bug]
Keep asking why until we find the root cause."
```

### Fault Tree Analysis

For complex bugs, map all possible causes:

```
Login Failure
â”œâ”€â”€ Authentication Issue
â”‚   â”œâ”€â”€ Invalid credentials
â”‚   â”‚   â”œâ”€â”€ Wrong password
â”‚   â”‚   â””â”€â”€ Account doesn't exist
â”‚   â”œâ”€â”€ Account locked
â”‚   â””â”€â”€ 2FA failure
â”œâ”€â”€ Token Issue
â”‚   â”œâ”€â”€ Token expired
â”‚   â”œâ”€â”€ Token malformed
â”‚   â””â”€â”€ Wrong signing key
â”œâ”€â”€ Infrastructure Issue
â”‚   â”œâ”€â”€ Database unreachable
â”‚   â”œâ”€â”€ Redis cache miss
â”‚   â””â”€â”€ Network timeout
â””â”€â”€ Client Issue
    â”œâ”€â”€ CORS blocked
    â”œâ”€â”€ Cookie not sent
    â””â”€â”€ JavaScript error
```

```bash
codex "Create a fault tree for this error: [error message]
List all possible causes organized by category."
```

### Strategic Instrumentation

When the bug is mysterious, add logging strategically:

```bash
codex "I need to debug why orders sometimes have wrong totals.
Where should I add logging to trace the calculation?"
```

Response:
```typescript
// Add logging at these decision points:

function calculateOrderTotal(order: Order): number {
  console.log('[CALC] Starting calculation for order:', order.id);

  let subtotal = 0;
  for (const item of order.items) {
    const itemTotal = item.price * item.quantity;
    console.log(`[CALC] Item ${item.id}: ${item.price} Ã— ${item.quantity} = ${itemTotal}`);
    subtotal += itemTotal;
  }
  console.log('[CALC] Subtotal:', subtotal);

  const discount = calculateDiscount(order);
  console.log('[CALC] Discount applied:', discount);

  const tax = calculateTax(subtotal - discount);
  console.log('[CALC] Tax:', tax);

  const total = subtotal - discount + tax;
  console.log('[CALC] Final total:', total);

  return total;
}

// After finding the bug, remove or convert to debug-level logging
```

---

## 6. Quality Patterns Library

### Pattern: Defense in Depth

Validate at multiple layersâ€”never trust data from the previous layer:

```typescript
// Layer 1: API Input (don't trust the client)
app.post('/users', async (req, res) => {
  const input = UserCreateSchema.parse(req.body); // Zod validation

  // Layer 2: Business Logic (don't trust the API layer)
  const user = await userService.create({
    email: input.email.toLowerCase().trim(),
    name: sanitizeHtml(input.name),
  });

  // Layer 3: Database (don't trust the service layer)
  // DB has constraints: unique email, NOT NULL, max lengths
});
```

### Pattern: Fail Fast with Context

Errors should scream useful information:

```typescript
// âŒ BAD: Silent or vague failure
function getUser(id) {
  const user = db.users.find(id);
  return user || null;  // Caller has no idea why it failed
}

// âœ… GOOD: Fail fast with context
function getUser(id: string): User {
  if (!id) {
    throw new Error('getUser requires an id');
  }

  const user = db.users.find(id);

  if (!user) {
    throw new NotFoundError(`User not found: ${id}`);
  }

  return user;
}
```

### Pattern: Invariant Assertions

Assert things that "should never happen"â€”they will:

```typescript
function processPayment(order: Order): Receipt {
  // These should never fail if upstream code is correct
  // But if they do, we want to know immediately
  invariant(order.total > 0, `Order total must be positive: ${order.total}`);
  invariant(order.items.length > 0, 'Order must have items');
  invariant(order.userId, 'Order must have a user');

  // Now process with confidence
  return paymentGateway.charge(order);
}

function invariant(condition: boolean, message: string): asserts condition {
  if (!condition) {
    throw new Error(`Invariant violation: ${message}`);
  }
}
```

---

## Key Takeaways

| Concept | Remember |
|---------|----------|
| **Multi-Layer Reviews** | Automated â†’ AI â†’ Human. Each catches different issues. |
| **TDD** | Red â†’ Green â†’ Refactor. Tests are design, not just verification. |
| **3-Attempt Rule** | Stop guessing after 3 tries. Go systematic. |
| **Quality Gates** | Each gate must pass before proceeding. No exceptions. |
| **Root Cause Analysis** | Fix causes, not symptoms. Ask "why" 5 times. |

---

## Try This Now

1. **Set up Layer 1**: Add formatting and linting hooks to your project
2. **Practice TDD**: Build a simple function (string formatter, validator) using Red-Green-Refactor
3. **Create your pre-PR script**: Write a `pre-pr-check.sh` that runs your quality checks
4. **Debug systematically**: Next bug you hit, use the 5 Whys before guessing

---

## Quick Reference

```bash
# AI Code Review
codex "Review my changes for: logic errors, edge cases, security issues"

# TDD Cycle
codex "Write a failing test for [feature]"  # RED
codex "Implement minimal code to pass"       # GREEN
codex "Refactor for clarity"                 # REFACTOR

# Systematic Debugging
codex "Help me trace backwards from this error: [stack trace]"
codex "Create a fault tree for: [symptom]"

# Pre-PR Check
codex "Review my PR for issues before I submit"
```

---

**Build quality in from the start!** â†’ [Module 5 Exercises](../exercises/05-quality/)
